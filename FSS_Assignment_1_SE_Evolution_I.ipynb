{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import pydriller\n",
    "from collections import defaultdict\n",
    "\n",
    "from numpy.f2py.crackfortran import verbose\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setup",
   "id": "7f7771ec2af15026"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "react_repo = \"https://github.com/facebook/react\"\n",
    "clone_dir = os.path.join(os.getcwd(), \"react\")\n",
    "\n",
    "if not exists(clone_dir):\n",
    "    with tqdm(total=100, desc=\"Cloning React repo\", unit=\"chunk\") as progress_bar:\n",
    "        process = subprocess.Popen(\n",
    "            ['git', 'clone', '--progress', react_repo, clone_dir],\n",
    "            stderr=subprocess.PIPE,\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            text=True\n",
    "        )\n",
    "        for line in process.stderr:\n",
    "            if \"Receiving objects\" in line:\n",
    "                percentage = int(line.split(\"%\")[0].split()[-1])\n",
    "                progress_bar.n = percentage\n",
    "                progress_bar.refresh()\n",
    "            elif \"Resolving deltas\" in line:\n",
    "                progress_bar.set_description(\"Resolving deltas\")\n",
    "                progress_bar.refresh()\n",
    "        process.wait()\n",
    "\n",
    "    if process.returncode == 0:\n",
    "        logging.info(\"cloning completed successfully\")\n",
    "    else:\n",
    "        logging.error(\"error during cloning\")\n",
    "else:\n",
    "    logging.warning(\"repo already cloned\")"
   ],
   "id": "db2924dbc9353d00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Task 1",
   "id": "594f9bd65731a2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Patterns to identify component types\n",
    "class_component_pattern = r\"class\\s+\\w+\\s+extends\\s+(React\\.Component|React\\.PureComponent)\"\n",
    "functional_component_pattern = r\"(function\\s+\\w+\\s*\\([^)]*\\)\\s*{[^}]*return\\s*<[^>]+>)|(\\w+\\s*=\\s*\\([^)]*\\)\\s*=>\\s*<[^>]+>)\"\n",
    "\n",
    "components = {\n",
    "    \"class_components\": [],\n",
    "    \"functional_components\": []\n",
    "}\n",
    "\n",
    "\n",
    "for root, _, files in os.walk(clone_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".js\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "                # Class Components\n",
    "                if re.search(class_component_pattern, content):\n",
    "                    components[\"class_components\"].append(file)\n",
    "                \n",
    "                # Functional Components\n",
    "                elif re.search(functional_component_pattern, content):\n",
    "                    components[\"functional_components\"].append(file)\n",
    "\n",
    "print(\"Class Components:\")\n",
    "print(json.dumps(components[\"class_components\"], indent=4))\n",
    "\n",
    "print(\"\\nFunctional Components:\")\n",
    "print(json.dumps(components[\"functional_components\"], indent=4))\n"
   ],
   "id": "fef57140856b7894",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "components_output_path = os.path.join(os.getcwd(), \"task1\", \"components.json\")\n",
    "os.makedirs(os.path.dirname(components_output_path), exist_ok=True)\n",
    "\n",
    "with open(components_output_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(components, json_file, indent=4)"
   ],
   "id": "39ddbf064242616c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#################################\n",
    "# Detect Dependencies with Madge\n",
    "#################################\n",
    "# Executing the Madge command\n",
    "logging.info(\"Running Madge to generate dependencies.json\")\n",
    "system(\"madge --json . > ./dependencies.json\")\n",
    "\n",
    "logging.info(f\"Directory: {os.getcwd()}\\\\\")\n",
    "\n",
    "if exists(\"./dependencies.json\"):\n",
    "    logging.info(\"Dependencies data has been saved to 'dependencies.json'\")\n",
    "else:\n",
    "    logging.error(\"Error generating the dependencies data\")\n",
    "    raise FileNotFoundError(\"dependencies.json file not found\")\n",
    "\n",
    "# Load dependencies JSON file \n",
    "with open(\"./dependencies.json\", 'r', encoding='utf-8') as f:\n",
    "    dependencies = json.load(f)\n",
    "\n",
    "# Calculate the number of dependencies for each file\n",
    "dependency_counts = {file: len(dependencies[file]) for file in dependencies}\n",
    "\n",
    "# Sort files by the number of dependencies\n",
    "top_3_files = sorted(dependency_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "# Prepare the top 3 files data\n",
    "top_dependencies_data = {}\n",
    "for file, _ in top_3_files:\n",
    "    top_dependencies_data[file] = dependencies[file]\n",
    "\n",
    "# Save only the top 3 files to a new JSON file\n",
    "top_dependencies_file = \"./top_dependencies.json\"\n",
    "with open(top_dependencies_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(top_dependencies_data, f, indent=4)\n",
    "\n",
    "# Check if the 'top_dependencies.json' file has been generated\n",
    "if exists(top_dependencies_file):\n",
    "    logging.info(f\"Top 3 files with the highest number of dependencies have been saved to '{top_dependencies_file}'\")\n",
    "else:\n",
    "    logging.error(f\"Error generating the top dependencies data in '{top_dependencies_file}'\")\n",
    "    raise FileNotFoundError(f\"{top_dependencies_file} file not found\")"
   ],
   "id": "4dbf38c9376c1f00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T21:20:41.307090Z",
     "start_time": "2024-11-21T21:20:41.250254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##########################\n",
    "# Changes between Versions\n",
    "##########################\n",
    "\n",
    "logging.info(\"Getting the list of commits between v17.0.1 and v17.0.2\")\n",
    "commit_hashes = subprocess.check_output(['git', 'log', 'v17.0.1..v17.0.2', '--pretty=format:%H'], text=True).splitlines()\n",
    "\n",
    "if not commit_hashes:\n",
    "    logging.error(\"No commits found between v17.0.1 and v17.0.2\")\n",
    "    raise ValueError(\"No commits found between v17.0.1 and v17.0.2\")\n",
    "\n",
    "# Data structure to hold commit information\n",
    "commit_info_list = []\n",
    "\n",
    "# Regex patterns for changes, insertions, and deletions\n",
    "files_changed_pattern = re.compile(r'(\\d+) file[s]? changed')\n",
    "insertions_pattern = re.compile(r'(\\d+) insertion[s]?\\(\\+\\)')\n",
    "deletions_pattern = re.compile(r'(\\d+) deletion[s]?\\(\\-\\)')\n",
    "\n",
    "# Iterate through each commit hash to extract inf\n",
    "for commit_hash in commit_hashes:\n",
    "    logging.info(f\"Processing commit {commit_hash}\")\n",
    "    commit_details = subprocess.check_output(['git', 'show', '--stat', '--pretty=format:', commit_hash], text=True)\n",
    "\n",
    "    # Extract information \n",
    "    files_changed_match = files_changed_pattern.search(commit_details)\n",
    "    insertions_match = insertions_pattern.search(commit_details)\n",
    "    deletions_match = deletions_pattern.search(commit_details)\n",
    "\n",
    "    # Extract values or default to 0 \n",
    "    files_changed = int(files_changed_match.group(1)) if files_changed_match else 0\n",
    "    insertions = int(insertions_match.group(1)) if insertions_match else 0\n",
    "    deletions = int(deletions_match.group(1)) if deletions_match else 0\n",
    "\n",
    "\n",
    "    commit_info_list.append({\n",
    "        \"commit_hash\": commit_hash,\n",
    "        \"files_changed\": files_changed,\n",
    "        \"insertions\": insertions,\n",
    "        \"deletions\": deletions\n",
    "    })\n",
    "\n",
    "# Most substantial change \n",
    "max_commit = max(commit_info_list, key=lambda x: x['files_changed'])\n",
    "    \n",
    "commit_hash_task3 = max_commit['commit_hash'] \n",
    "\n",
    "# Documentation \n",
    "logging.info(\"Commit with the most substantial change:\")\n",
    "logging.info(f\"Commit Hash: {max_commit['commit_hash']}\")\n",
    "logging.info(f\"Files Changed: {max_commit['files_changed']}\")\n",
    "logging.info(f\"Insertions: {max_commit['insertions']}\")\n",
    "logging.info(f\"Deletions: {max_commit['deletions']}\")\n",
    "\n",
    "# Save the commit information \n",
    "commit_info_path = \"./commit_info.json\"\n",
    "with open(commit_info_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(max_commit, f, indent=4)\n",
    "\n",
    "if exists(commit_info_path):\n",
    "    logging.info(f\"Commit information saved to '{commit_info_path}'\")\n",
    "else:\n",
    "    logging.error(f\"Failed to save commit information to '{commit_info_path}'\")\n"
   ],
   "id": "970c0116af1673b3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 22:20:41,255 | INFO | Getting the list of commits between v17.0.1 and v17.0.2\n",
      "2024-11-21 22:20:41,271 | INFO | Processing commit 12adaffef7105e2714f82651ea51936c563fe15c\n",
      "2024-11-21 22:20:41,286 | INFO | Processing commit b2bbee7ba31bb7d212a9ff2e682a695a32f8a87f\n",
      "2024-11-21 22:20:41,295 | INFO | Processing commit 8cc6ff24880ac00fdb9d11bce480a0433456e82d\n",
      "2024-11-21 22:20:41,304 | INFO | Commit with the most substantial change:\n",
      "2024-11-21 22:20:41,304 | INFO | Commit Hash: 12adaffef7105e2714f82651ea51936c563fe15c\n",
      "2024-11-21 22:20:41,304 | INFO | Files Changed: 4\n",
      "2024-11-21 22:20:41,305 | INFO | Insertions: 15\n",
      "2024-11-21 22:20:41,305 | INFO | Deletions: 123\n",
      "2024-11-21 22:20:41,305 | INFO | Commit information saved to './commit_info.json'\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "##########################\n",
    "# Dependency Change\n",
    "##########################\n",
    "\n",
    "try:\n",
    "    subprocess.run([\"git\", \"checkout\", commit_hash], check=True, text=True)\n",
    "    print(f\"Checked out to commit {commit_hash} successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error checking out to commit {commit_hash}: {e}\")"
   ],
   "id": "d4007d7a6fc08e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T21:21:29.090681Z",
     "start_time": "2024-11-21T21:21:29.074917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result_commit = subprocess.run(f\"madge --json ./ > 'dependencies_commit.json'\", shell=True)\n",
    "\n",
    "# Step 3: Load dependencies from dependencies.json\n",
    "with open(\"./task1/dependencies.json\", 'r', encoding='utf-8') as f:\n",
    "    try:\n",
    "        dependencies = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        dependencies = {}"
   ],
   "id": "ccb3be70f758c98c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: madge: command not found\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './task1/dependencies.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [12], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m result_commit \u001B[38;5;241m=\u001B[39m subprocess\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmadge --json ./ > \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdependencies_commit.json\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m, shell\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Step 3: Load dependencies from dependencies.json\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./task1/dependencies.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m      6\u001B[0m         dependencies \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './task1/dependencies.json'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Execute Madge command for v17.0.1\n",
    "subprocess.run([\"git\", \"checkout\", \"v17.0.1\"], shell=True)\n",
    "result_v17_0_1 = subprocess.run(f\"madge --json ./ > 'dependencies_v17_0_1.json'\", shell=True)\n",
    "\n",
    "with open(\"dependencies_v17_0_1.json\", 'r', encoding='utf-8') as f:\n",
    "    try:\n",
    "        dependencies = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        dependencies = {}\n",
    "\n",
    "\n",
    "# Execute Madge command for v17.0.2\n",
    "subprocess.run([\"git\", \"checkout\", \"v17.0.2\"], shell=True)\n",
    "result_v17_0_2 = subprocess.run(f\"madge --json ./ > 'dependencies_v17_0_2.json'\", shell=True)\n",
    "\n",
    "with open(\"dependencies_v17_0_2.json\", 'r', encoding='utf-8') as f:\n",
    "    try:\n",
    "        dependencies = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        dependencies = {}\n"
   ],
   "id": "48ba303690e1153a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compare dependency changes in v17_0_1 and v17_0_2\n",
    "\n",
    "if os.path.exists(\"dependencies_v17_0_1.json\") and os.path.exists(\"dependencies_v17_0_2.json\"):\n",
    "    with open(\"dependencies_v17_0_1.json\", 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            dependencies_v17_0_1 = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for v17.0.1: {e}\")\n",
    "            dependencies_v17_0_1 = {}\n",
    "\n",
    "    with open(\"dependencies_v17_0_2.json\", 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            dependencies_v17_0_2 = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for v17.0.2: {e}\")\n",
    "            dependencies_v17_0_2 = {}\n",
    "else:\n",
    "    dependencies_v17_0_1 = {}\n",
    "    dependencies_v17_0_2 = {}\n",
    "\n",
    "changes = {\n",
    "    \"new_dependencies\": {},\n",
    "    \"removed_dependencies\": {}\n",
    "}\n",
    "\n",
    "# Find new dependencies introduced in v17.0.2\n",
    "for file, deps in dependencies_v17_0_2.items():\n",
    "    if file not in dependencies_v17_0_1:\n",
    "        changes[\"new_dependencies\"][file] = deps\n",
    "    else:\n",
    "        new_deps = set(deps) - set(dependencies_v17_0_1[file])\n",
    "        if new_deps:\n",
    "            changes[\"new_dependencies\"][file] = list(new_deps)\n",
    "\n",
    "# Find dependencies that were removed in v17.0.2\n",
    "for file, deps in dependencies_v17_0_1.items():\n",
    "    if file not in dependencies_v17_0_2:\n",
    "        changes[\"removed_dependencies\"][file] = deps\n",
    "    else:\n",
    "        removed_deps = set(deps) - set(dependencies_v17_0_2[file])\n",
    "        if removed_deps:\n",
    "            changes[\"removed_dependencies\"][file] = list(removed_deps)\n",
    "\n",
    "# Save dependency changes to a new JSON file\n",
    "if changes[\"new_dependencies\"] or changes[\"removed_dependencies\"]:\n",
    "    with open(\"dependency_changes\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(changes, f, indent=4)\n",
    "    print(f\"Dependency changes between v17.0.1 and v17.0.2 have been documented in dependency_changes\")\n",
    "else:\n",
    "    print(\"No changes in dependencies detected between v17.0.1 and v17.0.2.\")"
   ],
   "id": "7b55f27777f37189",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Task 2",
   "id": "746521bd8d73fd35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mining Commit History\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "try:\n",
    "    commits_data = []\n",
    "    for commit in pydriller.Repository(clone_dir).traverse_commits():\n",
    "        commit_time = commit.committer_date\n",
    "        file_names = [mod.filename for mod in commit.modified_files]\n",
    "        \n",
    "        commits_data.append({\n",
    "            \"commit_time\": commit_time,\n",
    "            \"files\": file_names\n",
    "        })\n",
    "\n",
    "finally:\n",
    "    logging.getLogger().setLevel(logging.INFO)"
   ],
   "id": "def9f69695d9c90b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Temporal Coupling",
   "id": "523a8dd19675a65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Analysis of Temporal Coupling\n",
    "def divide_into_year_intervals(commits_data):\n",
    "    \"\"\"\n",
    "    Helper function to dissect commits_data into yearly intervals\n",
    "    \"\"\"\n",
    "    intervals = defaultdict(list)\n",
    "    for commit in commits_data:\n",
    "        year = commit[\"commit_time\"].year\n",
    "        intervals[year].append(commit)\n",
    "    return intervals\n",
    "\n",
    "# Yearly Temporal Couplings Analysis\n",
    "def analyze_temporal_coupling(commits_data, time_windows):\n",
    "    \"\"\"\n",
    "    Creates a dictionary for each year, containing another dictionary with the time_windows as keys and the corresponding coupling history.\n",
    "    \"\"\"\n",
    "    intervals = divide_into_year_intervals(commits_data)\n",
    "    yearly_temporal_couplings = defaultdict(list)\n",
    "\n",
    "    for year, commits in intervals.items():\n",
    "        temporal_couplings = {window: defaultdict(int) for window in time_windows}\n",
    "\n",
    "        for i, commit_i in enumerate(commits):\n",
    "            for j, commit_j in enumerate(commits):\n",
    "                if i >= j:  # Avoid duplicated entries\n",
    "                    continue\n",
    "\n",
    "                time_diff = abs((commit_j[\"commit_time\"] - commit_i[\"commit_time\"]).total_seconds() / 3600)\n",
    "\n",
    "                for window in time_windows:\n",
    "                    if time_diff <= window:\n",
    "                        for file1 in commit_i[\"files\"]:\n",
    "                            for file2 in commit_j[\"files\"]:\n",
    "                                if file1 != file2 and not (file1.endswith(\".md\") or file2.endswith(\".md\")): # Exclusion of .md files and self-pairs -> see pdf report for further details\n",
    "                                    pair = tuple(sorted([file1, file2]))\n",
    "                                    temporal_couplings[window][pair] += 1\n",
    "\n",
    "        yearly_temporal_couplings[year] = temporal_couplings\n",
    "\n",
    "    return yearly_temporal_couplings\n",
    "\n",
    "# Extract top 3 temporal coupling entries to JSON\n",
    "def save_results_and_collect_dataframe(yearly_temporal_couplings, time_windows, output_dir=\"./task2/tc\"):\n",
    "    \"\"\"\n",
    "    Saves top 3 temporal coupling entries for each year into a single JSON file per year,\n",
    "    including all time windows. Also returns a dataframe for analysis and pdf reporting.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for year, temporal_couplings in yearly_temporal_couplings.items():\n",
    "        year_results = []\n",
    "        \n",
    "        for window in time_windows:\n",
    "            top_coupled = sorted(\n",
    "                temporal_couplings[window].items(), key=lambda x: x[1], reverse=True\n",
    "            )[:3]  # Top 3 file pairs for each time window\n",
    "\n",
    "            for pair, count in top_coupled:\n",
    "                entry = {\n",
    "                    \"file_pair\": list(pair),\n",
    "                    \"coupled_commits\": {\n",
    "                        \"time_window\": window,\n",
    "                        \"commit_count\": count\n",
    "                    }\n",
    "                }\n",
    "                year_results.append(entry)\n",
    "\n",
    "                all_results.append({\n",
    "                    \"Year\": year,\n",
    "                    \"Time Window (h)\": window,\n",
    "                    \"File Pair\": f\"{pair[0]} & {pair[1]}\",\n",
    "                    \"Commit Count\": count\n",
    "                })\n",
    "\n",
    "        year_file_path = os.path.join(output_dir, f\"{year}.json\")\n",
    "        with open(year_file_path, \"w\") as f:\n",
    "            json.dump(year_results, f, indent=4)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df\n"
   ],
   "id": "3e2f06190826c5a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "######################\n",
    "# TEMPORAL COUPLING\n",
    "######################\n",
    "\n",
    "time_windows = [24, 48, 72]\n",
    "yearly_temporal_couplings = analyze_temporal_coupling(commits_data, time_windows)\n",
    "df_temporal_coupling = save_results_and_collect_dataframe(yearly_temporal_couplings, time_windows)"
   ],
   "id": "398900c317030fe1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Yearly Logical Couplings Analysis\n",
    "def analyze_logical_coupling(commits_data):\n",
    "    \"\"\"\n",
    "    Analyzes logical coupling by finding files frequently committed together,\n",
    "    excluding self-pairs.\n",
    "    \"\"\"\n",
    "    intervals = divide_into_year_intervals(commits_data)\n",
    "    yearly_logical_couplings = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for year, commits in intervals.items():\n",
    "        for commit in commits:\n",
    "            files = [file for file in commit[\"files\"] if not file.endswith(\".md\")]  # Exclude .md files -> see pdf report\n",
    "            for i, file1 in enumerate(files):\n",
    "                for j, file2 in enumerate(files):\n",
    "                    if i < j and file1 != file2:  # Avoid duplicate pairs and self-pairs\n",
    "                        pair = tuple(sorted([file1, file2]))\n",
    "                        yearly_logical_couplings[year][pair] += 1\n",
    "\n",
    "    return yearly_logical_couplings\n",
    "\n",
    "# Extract top 3 logical coupling entries to JSON\n",
    "def save_results_and_collect_dataframe(yearly_logical_couplings, output_dir=\"./task2/lc\"):\n",
    "    \"\"\"\n",
    "    Saves top 3 logical coupling entries for each year into a single JSON file per year.\n",
    "    Also returns a dataframe for analysis and pdf reporting.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for year, logical_couplings in yearly_logical_couplings.items():\n",
    "        year_results = []\n",
    "\n",
    "        top_coupled = sorted(\n",
    "            logical_couplings.items(), key=lambda x: x[1], reverse=True\n",
    "        )[:3]\n",
    "\n",
    "        for pair, count in top_coupled:\n",
    "            entry = {\n",
    "                \"file_pair\": list(pair),\n",
    "                \"commit_count\": count\n",
    "            }\n",
    "            year_results.append(entry)\n",
    "\n",
    "            all_results.append({\n",
    "                \"Year\": year,\n",
    "                \"File Pair\": f\"{pair[0]} & {pair[1]}\",\n",
    "                \"Commit Count\": count\n",
    "            })\n",
    "\n",
    "        year_file_path = os.path.join(output_dir, f\"{year}.json\")\n",
    "        with open(year_file_path, \"w\") as f:\n",
    "            json.dump(year_results, f, indent=4)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df"
   ],
   "id": "3fdd43052bfd84a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "######################\n",
    "# LOGICAL COUPLING\n",
    "######################\n",
    "\n",
    "yearly_logical_couplings = analyze_logical_coupling(commits_data)\n",
    "df_logical_coupling = save_results_and_collect_dataframe(yearly_logical_couplings)"
   ],
   "id": "ebe89492d085eb8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Task 3",
   "id": "d505b987733b8068"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "222bf1c3383c7873",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
