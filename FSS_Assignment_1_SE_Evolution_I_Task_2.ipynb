{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T20:27:19.091616Z",
     "start_time": "2024-11-24T20:27:18.747880Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import pydriller\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setup",
   "id": "7f7771ec2af15026"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:27:19.946930Z",
     "start_time": "2024-11-24T20:27:19.941189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "react_repo = \"https://github.com/facebook/react\"\n",
    "clone_dir = os.path.join(os.getcwd(), \"react\")\n",
    "\n",
    "if not exists(clone_dir):\n",
    "    with tqdm(total=100, desc=\"Cloning React repo\", unit=\"chunk\") as progress_bar:\n",
    "        process = subprocess.Popen(\n",
    "            ['git', 'clone', '--progress', react_repo, clone_dir],\n",
    "            stderr=subprocess.PIPE,\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            text=True\n",
    "        )\n",
    "        for line in process.stderr:\n",
    "            if \"Receiving objects\" in line:\n",
    "                percentage = int(line.split(\"%\")[0].split()[-1])\n",
    "                progress_bar.n = percentage\n",
    "                progress_bar.refresh()\n",
    "            elif \"Resolving deltas\" in line:\n",
    "                progress_bar.set_description(\"Resolving deltas\")\n",
    "                progress_bar.refresh()\n",
    "        process.wait()\n",
    "\n",
    "    if process.returncode == 0:\n",
    "        logging.info(\"cloning completed successfully\")\n",
    "    else:\n",
    "        logging.error(\"error during cloning\")\n",
    "else:\n",
    "    logging.warning(\"repo already cloned\")"
   ],
   "id": "db2924dbc9353d00",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 21:27:19,945 | WARNING | repo already cloned\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Task 2",
   "id": "746521bd8d73fd35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:30:21.569120Z",
     "start_time": "2024-11-24T20:27:22.434375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Mining Commit History\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "try:\n",
    "    commits_data = []\n",
    "    for commit in pydriller.Repository(clone_dir).traverse_commits():\n",
    "        commit_time = commit.committer_date\n",
    "        file_names = [mod.filename for mod in commit.modified_files]\n",
    "        \n",
    "        commits_data.append({\n",
    "            \"commit_time\": commit_time,\n",
    "            \"files\": file_names\n",
    "        })\n",
    "\n",
    "finally:\n",
    "    logging.getLogger().setLevel(logging.INFO)"
   ],
   "id": "def9f69695d9c90b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Temporal Coupling",
   "id": "523a8dd19675a65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:30:21.587219Z",
     "start_time": "2024-11-24T20:30:21.578595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analysis of Temporal Coupling\n",
    "def divide_into_year_intervals(commits_data):\n",
    "    \"\"\"\n",
    "    Helper function to dissect commits_data into yearly intervals\n",
    "    \"\"\"\n",
    "    intervals = defaultdict(list)\n",
    "    for commit in commits_data:\n",
    "        year = commit[\"commit_time\"].year\n",
    "        intervals[year].append(commit)\n",
    "    return intervals\n",
    "\n",
    "# Yearly Temporal Couplings Analysis\n",
    "def analyze_temporal_coupling(commits_data, time_windows):\n",
    "    \"\"\"\n",
    "    Creates a dictionary for each year, containing another dictionary with the time_windows as keys and the corresponding coupling history.\n",
    "    \"\"\"\n",
    "    intervals = divide_into_year_intervals(commits_data)\n",
    "    yearly_temporal_couplings = defaultdict(list)\n",
    "\n",
    "    for year, commits in intervals.items():\n",
    "        temporal_couplings = {window: defaultdict(int) for window in time_windows}\n",
    "\n",
    "        for i, commit_i in enumerate(commits):\n",
    "            for j, commit_j in enumerate(commits):\n",
    "                if i >= j:  # Avoid duplicated entries\n",
    "                    continue\n",
    "\n",
    "                time_diff = abs((commit_j[\"commit_time\"] - commit_i[\"commit_time\"]).total_seconds() / 3600)\n",
    "\n",
    "                for window in time_windows:\n",
    "                    if time_diff <= window:\n",
    "                        for file1 in commit_i[\"files\"]:\n",
    "                            for file2 in commit_j[\"files\"]:\n",
    "                                if file1 != file2 and (file1.endswith(\".js\") and file2.endswith(\".js\")): # Exclusion of .md files and self-pairs -> see pdf report for further details\n",
    "                                    pair = tuple(sorted([file1, file2]))\n",
    "                                    temporal_couplings[window][pair] += 1\n",
    "\n",
    "        yearly_temporal_couplings[year] = temporal_couplings\n",
    "\n",
    "    return yearly_temporal_couplings\n",
    "\n",
    "\n",
    "# Extract top 3 temporal coupling entries to JSON\n",
    "def save_results_and_collect_dataframe(yearly_temporal_couplings, time_windows, output_dir=\"./task2/tc\"):\n",
    "    \"\"\"\n",
    "    Saves top 3 temporal coupling entries for each year into a single JSON file per year,\n",
    "    including all time windows. Also returns a dataframe for analysis and pdf reporting.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for year, temporal_couplings in yearly_temporal_couplings.items():\n",
    "        year_results = []\n",
    "        \n",
    "        for window in time_windows:\n",
    "            top_coupled = sorted(\n",
    "                temporal_couplings[window].items(), key=lambda x: x[1], reverse=True\n",
    "            )[:3]  # Top 3 file pairs for each time window\n",
    "\n",
    "            for pair, count in top_coupled:\n",
    "                entry = {\n",
    "                    \"file_pair\": list(pair),\n",
    "                    \"coupled_commits\": {\n",
    "                        \"time_window\": window,\n",
    "                        \"commit_count\": count\n",
    "                    }\n",
    "                }\n",
    "                year_results.append(entry)\n",
    "\n",
    "                all_results.append({\n",
    "                    \"Year\": year,\n",
    "                    \"Time Window (h)\": window,\n",
    "                    \"File Pair\": f\"{pair[0]} & {pair[1]}\",\n",
    "                    \"Commit Count\": count\n",
    "                })\n",
    "\n",
    "        year_file_path = os.path.join(output_dir, f\"{year}.json\")\n",
    "        with open(year_file_path, \"w\") as f:\n",
    "            json.dump(year_results, f, indent=4)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_temporal_coupling_overall(commits_data, time_windows):\n",
    "    \"\"\"\n",
    "    Analyzes temporal coupling over the entire commit history without dissecting into yearly intervals.\n",
    "    \"\"\"\n",
    "    temporal_couplings = {window: defaultdict(int) for window in time_windows}\n",
    "\n",
    "    for i, commit_i in enumerate(commits_data):\n",
    "        for j, commit_j in enumerate(commits_data):\n",
    "            if i >= j:  # Avoid duplicated entries and self-comparison\n",
    "                continue\n",
    "\n",
    "            time_diff = abs((commit_j[\"commit_time\"] - commit_i[\"commit_time\"]).total_seconds() / 3600)\n",
    "\n",
    "            for window in time_windows:\n",
    "                if time_diff <= window:\n",
    "                    for file1 in commit_i[\"files\"]:\n",
    "                        for file2 in commit_j[\"files\"]:\n",
    "                            if file1 != file2 and (file1.endswith(\".js\") and file2.endswith(\".js\")):  # Exclude non-JS files and self-pairs\n",
    "                                pair = tuple(sorted([file1, file2]))\n",
    "                                temporal_couplings[window][pair] += 1\n",
    "\n",
    "    return temporal_couplings\n",
    "\n",
    "\n",
    "# Extract top 3 temporal coupling entries to JSON\n",
    "def save_overall_results(temporal_couplings, time_windows, output_file=\"./task2/tc/overall.json\"):\n",
    "    \"\"\"\n",
    "    Saves top 3 temporal coupling entries for the entire commit history into a JSON file,\n",
    "    including all time windows. Also returns a dataframe for analysis and reporting.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(output_file)):\n",
    "        os.makedirs(os.path.dirname(output_file))\n",
    "\n",
    "    overall_results = []\n",
    "\n",
    "    for window in time_windows:\n",
    "        top_coupled = sorted(\n",
    "            temporal_couplings[window].items(), key=lambda x: x[1], reverse=True\n",
    "        )[:3]  # Top 3 file pairs for each time window\n",
    "\n",
    "        for pair, count in top_coupled:\n",
    "            entry = {\n",
    "                \"file_pair\": list(pair),\n",
    "                \"coupled_commits\": {\n",
    "                    \"time_window\": window,\n",
    "                    \"commit_count\": count\n",
    "                }\n",
    "            }\n",
    "            overall_results.append(entry)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(overall_results, f, indent=4)\n",
    "\n",
    "    df = pd.DataFrame([{\n",
    "        \"Time Window (h)\": entry[\"coupled_commits\"][\"time_window\"],\n",
    "        \"File Pair\": f\"{entry['file_pair'][0]} & {entry['file_pair'][1]}\",\n",
    "        \"Commit Count\": entry[\"coupled_commits\"][\"commit_count\"]\n",
    "    } for entry in overall_results])\n",
    "\n",
    "    return df"
   ],
   "id": "3e2f06190826c5a1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:32:41.403108Z",
     "start_time": "2024-11-24T20:30:21.595751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "######################\n",
    "# TEMPORAL COUPLING\n",
    "######################\n",
    "\n",
    "time_windows = [24, 48, 72]\n",
    "\n",
    "# Yearly Analysis\n",
    "yearly_temporal_couplings = analyze_temporal_coupling(commits_data, time_windows)\n",
    "df_temporal_coupling = save_results_and_collect_dataframe(yearly_temporal_couplings, time_windows)\n",
    "\n",
    "# Overall Analysis\n",
    "temporal_couplings = analyze_temporal_coupling_overall(commits_data, time_windows)\n",
    "df_overall = save_overall_results(temporal_couplings, time_windows)"
   ],
   "id": "398900c317030fe1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:35:38.226875Z",
     "start_time": "2024-11-24T20:35:38.217416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Yearly Logical Couplings Analysis\n",
    "def analyze_logical_coupling(commits_data):\n",
    "    \"\"\"\n",
    "    Analyzes logical coupling by finding files frequently committed together,\n",
    "    excluding self-pairs.\n",
    "    \"\"\"\n",
    "    intervals = divide_into_year_intervals(commits_data)\n",
    "    yearly_logical_couplings = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for year, commits in intervals.items():\n",
    "        for commit in commits:\n",
    "            files = [file for file in commit[\"files\"] if file.endswith(\".js\")]  # Exclude .md files -> see pdf report\n",
    "            for i, file1 in enumerate(files):\n",
    "                for j, file2 in enumerate(files):\n",
    "                    if i < j and file1 != file2:  # Avoid duplicate pairs and self-pairs\n",
    "                        pair = tuple(sorted([file1, file2]))\n",
    "                        yearly_logical_couplings[year][pair] += 1\n",
    "\n",
    "    return yearly_logical_couplings\n",
    "\n",
    "# Extract top 3 logical coupling entries to JSON\n",
    "def save_results_and_collect_dataframe(yearly_logical_couplings, output_dir=\"./task2/lc\"):\n",
    "    \"\"\"\n",
    "    Saves top 3 logical coupling entries for each year into a single JSON file per year.\n",
    "    Also returns a dataframe for analysis and pdf reporting.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for year, logical_couplings in yearly_logical_couplings.items():\n",
    "        year_results = []\n",
    "\n",
    "        top_coupled = sorted(\n",
    "            logical_couplings.items(), key=lambda x: x[1], reverse=True\n",
    "        )[:3]\n",
    "\n",
    "        for pair, count in top_coupled:\n",
    "            entry = {\n",
    "                \"file_pair\": list(pair),\n",
    "                \"commit_count\": count\n",
    "            }\n",
    "            year_results.append(entry)\n",
    "\n",
    "            all_results.append({\n",
    "                \"Year\": year,\n",
    "                \"File Pair\": f\"{pair[0]} & {pair[1]}\",\n",
    "                \"Commit Count\": count\n",
    "            })\n",
    "\n",
    "        year_file_path = os.path.join(output_dir, f\"{year}.json\")\n",
    "        with open(year_file_path, \"w\") as f:\n",
    "            json.dump(year_results, f, indent=4)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_logical_coupling_overall(commits_data):\n",
    "    \"\"\"\n",
    "    Analyzes logical coupling over the entire commit history by finding files frequently committed together,\n",
    "    excluding self-pairs.\n",
    "    \"\"\"\n",
    "    logical_couplings = defaultdict(int)\n",
    "\n",
    "    for commit in commits_data:\n",
    "        files = [file for file in commit[\"files\"] if file.endswith(\".js\")]  # Exclude non-JS files\n",
    "        for i, file1 in enumerate(files):\n",
    "            for j, file2 in enumerate(files):\n",
    "                if i < j and file1 != file2:  # Avoid duplicate pairs and self-pairs\n",
    "                    pair = tuple(sorted([file1, file2]))\n",
    "                    logical_couplings[pair] += 1\n",
    "\n",
    "    return logical_couplings\n",
    "\n",
    "# Extract top 3 logical coupling entries to JSON\n",
    "def save_overall_logical_couplings(logical_couplings, output_file=\"./task2/lc/overall.json\"):\n",
    "    \"\"\"\n",
    "    Saves top 3 logical coupling entries for the entire commit history into a JSON file.\n",
    "    Also returns a dataframe for analysis and reporting.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(output_file)):\n",
    "        os.makedirs(os.path.dirname(output_file))\n",
    "\n",
    "    # Get the top 3 file pairs with the highest commit counts\n",
    "    top_coupled = sorted(\n",
    "        logical_couplings.items(), key=lambda x: x[1], reverse=True\n",
    "    )[:3]\n",
    "\n",
    "    overall_results = []\n",
    "\n",
    "    for pair, count in top_coupled:\n",
    "        entry = {\n",
    "            \"file_pair\": list(pair),\n",
    "            \"commit_count\": count\n",
    "        }\n",
    "        overall_results.append(entry)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(overall_results, f, indent=4)\n",
    "\n",
    "    df = pd.DataFrame([{\n",
    "        \"File Pair\": f\"{entry['file_pair'][0]} & {entry['file_pair'][1]}\",\n",
    "        \"Commit Count\": entry[\"commit_count\"]\n",
    "    } for entry in overall_results])\n",
    "\n",
    "    return df"
   ],
   "id": "3fdd43052bfd84a5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:35:45.964113Z",
     "start_time": "2024-11-24T20:35:42.095711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "######################\n",
    "# LOGICAL COUPLING\n",
    "######################\n",
    "\n",
    "# Yearly\n",
    "yearly_logical_couplings = analyze_logical_coupling(commits_data)\n",
    "df_logical_coupling = save_results_and_collect_dataframe(yearly_logical_couplings)\n",
    "\n",
    "# Overall\n",
    "logical_couplings = analyze_logical_coupling_overall(commits_data)\n",
    "df_overall_lc = save_overall_logical_couplings(logical_couplings)"
   ],
   "id": "ebe89492d085eb8e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c9a4a63ab4e3a1a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
